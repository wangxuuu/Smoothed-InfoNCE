{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "from src.utils import *\n",
    "from src.mi_estimators import *\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "torch.set_default_tensor_type(FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--rho\", type=float, default=0.9, help=\"coefficient of Gaussian\")\n",
    "parser.add_argument(\"--d\", type=int, default=128, help=\"dimension of X & Y\")\n",
    "parser.add_argument(\"--sample_size\", type=int, default=400, help=\"sample size\")\n",
    "parser.add_argument(\"--gamma\", type=float, default=1e-10, help=\"clipping parameter\")\n",
    "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=40, help=\"size of the batches\")\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-4, help=\"adam: learning rate\")\n",
    "parser.add_argument(\"--hidden_dim\", type=int, default=100, help=\"Hidden dimension\")\n",
    "parser.add_argument(\"--ma_rate\", type=float, default=0.1, help=\"move average rate\")\n",
    "parser.add_argument(\"--ma_ef\", type=float, default=1, help=\"move average ef\")\n",
    "parser.add_argument(\"--alpha\", type=float, default=1e-4, help=\"smooth parameter\")\n",
    "parser.add_argument(\"--reg\", type=int, default=1, help=\"if apply regularization\")\n",
    "parser.add_argument(\"--n_epoch\", type=int, default=1000, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--n_iters_1epoch\", type=int, default=10, help=\"number of epochs of training\")\n",
    "\n",
    "opt, unknown = parser.parse_known_args()\n",
    "opt.n_iters = opt.n_epoch * opt.n_iters_1epoch\n",
    "ma_rate = 0.01  # moving average rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "continue_train = True  # set to True to continue to train\n",
    "load_available = False # set to False to prevent loading previous results\n",
    "overwrite = False  # set to True to overwrite previously stored results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = GaussianData(opt.sample_size, d=opt.d, rho=opt.rho)\n",
    "X, Y, XY, Ground_truth = data.X, data.Y, torch.cat((data.X, data.Y), dim=1), data.mutual_information()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use n*(n-1) samples to train DT\n",
    "# x_tile = X.unsqueeze(0).repeat((opt.sample_size, 1, 1))\n",
    "# y_tile = Y.unsqueeze(1).repeat((1, opt.sample_size, 1))\n",
    "# train_data = torch.cat([x_tile, y_tile], dim = -1).reshape(-1, opt.d*2)\n",
    "# train_label = torch.eye(x_data.shape[0]).reshape(-1,1)\n",
    "\n",
    "# choose n marginal samples to train DT\n",
    "ref_X, ref_Y = shuffle_data(X, Y, opt.sample_size)\n",
    "ref_XY = torch.cat([ref_X, ref_Y], dim = 1)\n",
    "train_data = torch.cat([XY, ref_XY], dim = 0)\n",
    "train_label = torch.cat([torch.ones([opt.sample_size,1]), torch.zeros([opt.sample_size, 1])], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=20, min_samples_leaf=20)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(min_samples_leaf=20, max_depth=20)\n",
    "# clf = DecisionTreeClassifier(min_samples_leaf=400, max_depth=10)\n",
    "# clf = RandomForestClassifier(n_estimators=10, min_samples_split=5)\n",
    "clf.fit(train_data.cpu().numpy(),  train_label.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.004587301587301584, max_depth=20,\n",
       "                       min_samples_leaf=20)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = clf.cost_complexity_pruning_path(train_data.cpu().numpy(), train_label.cpu().numpy())\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "ccp = ccp_alphas[round(len(ccp_alphas)/2)]\n",
    "\n",
    "# clf = DecisionTreeClassifier(min_samples_leaf=5, max_depth=8)\n",
    "clf = DecisionTreeClassifier(min_samples_leaf=20, max_depth=20, ccp_alpha=ccp)\n",
    "# clf = RandomForestClassifier(n_estimators=10, min_samples_split=5)\n",
    "clf.fit(train_data.cpu().numpy(),  train_label.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7375"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(train_data.cpu().numpy(),  train_label.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the data needs to train and predict the label\n",
    "x_tile = X.unsqueeze(0).repeat((opt.sample_size, 1, 1))\n",
    "y_tile = Y.unsqueeze(1).repeat((1, opt.sample_size, 1))\n",
    "data_matrix = torch.cat([x_tile, y_tile], dim = -1)\n",
    "DT_prob_matrix = torch.Tensor(clf.predict_proba(data_matrix.reshape(-1, opt.d*2).cpu().numpy())[:,1].reshape(opt.sample_size, opt.sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f'./results/Smoothed_InfoNCE_dim{opt.d}_reg{opt.reg}_alpha{opt.alpha}'   # filename\n",
    "chkpt_name = name+'.pt'      # checkpoint\n",
    "\n",
    "from datetime import datetime\n",
    "TIMESTAMP = \"{0:%Y-%m-%dT%H-%M-%S/}\".format(datetime.now())\n",
    "writer = SummaryWriter(f'./results/log/Merge_dim{opt.d}_alpha{opt.alpha}/{TIMESTAMP}')\n",
    "model = Prob_Net(opt.d*2, opt.hidden_dim, sigma=0.02)\n",
    "# model = Prob_Net(opt.d+opt.d, hidden_size=opt.hidden_dim)\n",
    "ce_loss = torch.nn.BCELoss()\n",
    "mi_est_values = []\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), opt.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_available = True # set to False to prevent loading previous results\n",
    "if load_available and os.path.exists(chkpt_name):\n",
    "    checkpoint = torch.load(\n",
    "        chkpt_name, map_location='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    mi_list = checkpoint['mi_list']\n",
    "    model_state = checkpoint['model_state']\n",
    "    model.load_state_dict(model_state)\n",
    "    print('Previous results loaded.')\n",
    "else:\n",
    "    mi_list = [] # storing the mi estimation of each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randerange(*args, **kwargs):\n",
    "    s = np.random.permutation(*args, **kwargs)\n",
    "    t = np.random.permutation(*args, **kwargs)\n",
    "    b = s != t\n",
    "    return s[b], t[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue_train = False  # set to True to continue to train\n",
    "if continue_train:\n",
    "    _iter = 0\n",
    "    for i in range(opt.n_epoch):\n",
    "        idx = np.random.permutation(opt.sample_size)\n",
    "        idx_X, idx_Y = randerange(opt.sample_size)\n",
    "        for j in range(opt.n_iters_1epoch):\n",
    "            batch_idx = idx[j::opt.n_iters_1epoch]\n",
    "            batch_XY = XY[batch_idx]\n",
    "            batch_X_ref = X[idx_X[j::opt.n_iters_1epoch]]\n",
    "            batch_Y_ref = Y[idx_Y[j::opt.n_iters_1epoch]]\n",
    "            batch_XY_ref = torch.cat((batch_X_ref, batch_Y_ref), dim=1)\n",
    "            train_batch_XY = torch.cat((batch_XY, batch_XY_ref), dim=0)\n",
    "\n",
    "            prob_pos = DT_prob_matrix[idx[j::opt.n_iters_1epoch], idx[j::opt.n_iters_1epoch]]\n",
    "            prob_neg = DT_prob_matrix[idx_X[j::opt.n_iters_1epoch], idx_Y[j::opt.n_iters_1epoch]]\n",
    "            prob_batch = torch.cat([prob_pos, prob_neg], dim=0).reshape(-1,1)\n",
    "            hard_label_ = torch.cat((torch.ones(prob_pos.shape[0]), torch.zeros(prob_neg.shape[0]))).reshape(-1,1)\n",
    "\n",
    "            label = opt.alpha*prob_batch.reshape(-1,1) + hard_label_*(1-opt.alpha)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred_Y = model(train_batch_XY)\n",
    "            loss = ce_loss(pred_Y, label)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            mi_est = mi_estimate(model, XY, opt.gamma, 1)\n",
    "        mi_list.append(mi_est.item())\n",
    "\n",
    "        writer.add_scalar('mi_list', mi_est.item(), _iter)\n",
    "        writer.add_scalar('loss', loss, _iter)\n",
    "        _iter += 1\n",
    "        if _iter%200==0:\n",
    "            print(\"Iternation: %d, loss: %f, mi_est: %f\"%(_iter, loss.item(), mi_est))\n",
    "            fig = plot_fig(model, X, Y, opt.d if opt.d<6 else 6)\n",
    "            writer.add_figure('heatmap', fig, _iter)\n",
    "            writer.add_histogram('first layer', model.fc[0].weight.data, _iter)\n",
    "            writer.add_histogram('second layer', model.fc[1].weight.data, _iter)\n",
    "            writer.add_histogram('third layer', model.fc[2].weight.data, _iter)\n",
    "\n",
    "            writer.add_histogram('first layer (grad)', model.fc[0].weight.grad.data, _iter)\n",
    "            writer.add_histogram('second layer (grad)', model.fc[1].weight.grad.data, _iter)\n",
    "            writer.add_histogram('third layer (grad)', model.fc[2].weight.grad.data, _iter)\n",
    "\n",
    "writer.add_graph(model, (XY,))\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52b3e5fe9304ee37ed6fc02b58fb2a8a470f895336a7b69fe331a7c2f400c80f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('torch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
